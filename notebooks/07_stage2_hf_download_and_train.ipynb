{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 Training - Download Stage 1 from HuggingFace\n",
    "\n",
    "This notebook:\n",
    "1. Downloads Stage 1 classification model from HuggingFace\n",
    "2. Prepares Stage 2 training data with correct input shape\n",
    "3. Trains Stage 2 model (HH/LH/HL/LL classification)\n",
    "4. Evaluates performance\n",
    "\n",
    "Working with: **BTCUSDT 15m** (single symbol test)\n",
    "Can be extended to all 22 symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "# Configuration\n",
    "SYMBOL = 'BTCUSDT'\n",
    "TIMEFRAME = '15m'\n",
    "SYMBOL_SHORT = 'BTC'\n",
    "HF_DATASET_ID = 'zongowo111/v2-crypto-ohlcv-data'\n",
    "HF_MODEL_PATH = f'v1_model/{SYMBOL}/{TIMEFRAME}'\n",
    "HF_DATA_PATH = f'klines/{SYMBOL}/{SYMBOL_SHORT}_{TIMEFRAME}.parquet'\n",
    "\n",
    "# Stage 1 Model Architecture Parameters\n",
    "STAGE1_SEQUENCE_LENGTH = 10  # 10 bars\n",
    "STAGE1_NUM_FEATURES = 20     # 20 features per bar\n",
    "\n",
    "# Local paths\n",
    "DATA_DIR = Path('../data')\n",
    "STAGE2_DATA_DIR = DATA_DIR / 'stage2' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "MODEL_DIR = Path('../models')\n",
    "STAGE1_MODEL_DIR = MODEL_DIR / 'stage1' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "STAGE2_MODEL_DIR = MODEL_DIR / 'stage2' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "\n",
    "# Create directories\n",
    "STAGE2_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STAGE1_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STAGE2_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Symbol: {SYMBOL}')\n",
    "print(f'Timeframe: {TIMEFRAME}')\n",
    "print(f'Stage 1 Input Shape: ({STAGE1_SEQUENCE_LENGTH}, {STAGE1_NUM_FEATURES})')\n",
    "print(f'Stage 1 Model Dir: {STAGE1_MODEL_DIR}')\n",
    "print(f'Stage 2 Data Dir: {STAGE2_DATA_DIR}')\n",
    "print(f'Stage 2 Model Dir: {STAGE2_MODEL_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Stage 1 Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(f'Downloading Stage 1 model from HuggingFace...')\n",
    "print(f'Dataset: {HF_DATASET_ID}')\n",
    "print(f'Path: {HF_MODEL_PATH}')\n",
    "\n",
    "# Download classification.h5\n",
    "try:\n",
    "    classification_path = hf_hub_download(\n",
    "        repo_id=HF_DATASET_ID,\n",
    "        filename=f'{HF_MODEL_PATH}/classification.h5',\n",
    "        repo_type='dataset',\n",
    "        cache_dir=str(STAGE1_MODEL_DIR)\n",
    "    )\n",
    "    print(f'✓ Downloaded classification.h5: {classification_path}')\n",
    "except Exception as e:\n",
    "    print(f'Error downloading classification.h5: {e}')\n",
    "\n",
    "# Download params.json\n",
    "try:\n",
    "    params_path = hf_hub_download(\n",
    "        repo_id=HF_DATASET_ID,\n",
    "        filename=f'{HF_MODEL_PATH}/params.json',\n",
    "        repo_type='dataset',\n",
    "        cache_dir=str(STAGE1_MODEL_DIR)\n",
    "    )\n",
    "    print(f'✓ Downloaded params.json: {params_path}')\n",
    "except Exception as e:\n",
    "    print(f'Error downloading params.json: {e}')\n",
    "\n",
    "# List downloaded files\n",
    "print(f'\\nDownloaded files:')\n",
    "for file in STAGE1_MODEL_DIR.rglob('*'):\n",
    "    if file.is_file():\n",
    "        size = file.stat().st_size / 1024 / 1024\n",
    "        print(f'  - {file.name} ({size:.2f} MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Verify Stage 1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Find classification.h5\n",
    "classification_files = list(STAGE1_MODEL_DIR.rglob('classification.h5'))\n",
    "if not classification_files:\n",
    "    raise FileNotFoundError(f'No classification.h5 found in {STAGE1_MODEL_DIR}')\n",
    "\n",
    "stage1_model_path = classification_files[0]\n",
    "print(f'Loading Stage 1 model from: {stage1_model_path}')\n",
    "\n",
    "# Load model\n",
    "stage1_model = keras.models.load_model(str(stage1_model_path))\n",
    "print(f'✓ Model loaded successfully')\n",
    "\n",
    "# Get model input shape\n",
    "input_shape = stage1_model.input_shape\n",
    "print(f'\\nModel Input Shape: {input_shape}')\n",
    "print(f'Model Summary:')\n",
    "stage1_model.summary()\n",
    "\n",
    "# Verify expected shape\n",
    "if len(input_shape) == 3:  # (batch, seq_len, features)\n",
    "    STAGE1_SEQUENCE_LENGTH = input_shape[1]\n",
    "    STAGE1_NUM_FEATURES = input_shape[2]\n",
    "    print(f'\\n✓ Detected 3D input: sequence_length={STAGE1_SEQUENCE_LENGTH}, num_features={STAGE1_NUM_FEATURES}')\n",
    "else:\n",
    "    raise ValueError(f'Expected 3D input (batch, seq, features), got {len(input_shape)}D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'Downloading training data from HuggingFace...')\n",
    "print(f'Path: {HF_DATA_PATH}')\n",
    "\n",
    "try:\n",
    "    data_file = hf_hub_download(\n",
    "        repo_id=HF_DATASET_ID,\n",
    "        filename=HF_DATA_PATH,\n",
    "        repo_type='dataset',\n",
    "        cache_dir=str(DATA_DIR)\n",
    "    )\n",
    "    print(f'✓ Downloaded data: {data_file}')\n",
    "    data_file = Path(data_file)\n",
    "except Exception as e:\n",
    "    print(f'Error downloading data: {e}')\n",
    "    raise\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(data_file)\n",
    "print(f'\\nData loaded: {df.shape[0]} rows, {df.shape[1]} columns')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering & ZigZag Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import from src\n",
    "from src.zigzag_indicator import ZigZagIndicator\n",
    "from src.features import FeatureEngineer\n",
    "from src.utils import time_series_split\n",
    "\n",
    "print('Applying ZigZag indicator...')\n",
    "zigzag = ZigZagIndicator(depth=12, deviation=5, backstep=2)\n",
    "df = zigzag.label_kbars(df)\n",
    "print(f'✓ ZigZag labels applied')\n",
    "\n",
    "# Check label distribution\n",
    "print(f'\\nLabel distribution:')\n",
    "print(df['zigzag_label'].value_counts().sort_index())\n",
    "\n",
    "# Feature engineering\n",
    "print(f'\\nCalculating technical indicators...')\n",
    "fe = FeatureEngineer(lookback_periods=[5, 10, 20, 50, 200])\n",
    "df = fe.calculate_all_features(df)\n",
    "print(f'✓ Features calculated')\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = fe.get_feature_columns(df)\n",
    "if 'symbol' in feature_cols:\n",
    "    feature_cols.remove('symbol')\n",
    "\n",
    "print(f'Total features: {len(feature_cols)}')\n",
    "print(f'Feature columns: {feature_cols}')\n",
    "\n",
    "# Handle missing values\n",
    "df[feature_cols] = df[feature_cols].fillna(method='ffill').fillna(0)\n",
    "print(f'✓ Missing values handled')\n",
    "\n",
    "print(f'\\nFinal data shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create 3D Time Series Features for Stage 1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_3d_sequences(X, y, seq_length=10):\n",
    "    \"\"\"\n",
    "    Convert 2D features to 3D sequences for LSTM/CNN models.\n",
    "    \\n",
    "    Args:\n",
    "        X: Features (n_samples, n_features)\n",
    "        y: Labels (n_samples,)\n",
    "        seq_length: Window length for sequences\n",
    "    \\n",
    "    Returns:\n",
    "        X_seq: 3D sequences (n_sequences, seq_length, n_features)\n",
    "        y_seq: Corresponding labels (n_sequences,)\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])  # Label for end of sequence\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "print(f'Creating 3D sequences with sequence_length={STAGE1_SEQUENCE_LENGTH}...')\n",
    "\n",
    "# Split before creating sequences (to prevent data leakage)\n",
    "train_df, val_df, test_df = time_series_split(df, train_ratio=0.7, validation_ratio=0.15)\n",
    "\n",
    "# === Train Set ===\n",
    "X_train_2d = train_df[feature_cols].values\n",
    "y_train_2d = train_df['zigzag_label'].values\n",
    "X_train_3d, y_train_3d = create_3d_sequences(X_train_2d, y_train_2d, seq_length=STAGE1_SEQUENCE_LENGTH)\n",
    "print(f'Train: {X_train_3d.shape}')\n",
    "\n",
    "# === Validation Set ===\n",
    "X_val_2d = val_df[feature_cols].values\n",
    "y_val_2d = val_df['zigzag_label'].values\n",
    "X_val_3d, y_val_3d = create_3d_sequences(X_val_2d, y_val_2d, seq_length=STAGE1_SEQUENCE_LENGTH)\n",
    "print(f'Val: {X_val_3d.shape}')\n",
    "\n",
    "# === Test Set ===\n",
    "X_test_2d = test_df[feature_cols].values\n",
    "y_test_2d = test_df['zigzag_label'].values\n",
    "X_test_3d, y_test_3d = create_3d_sequences(X_test_2d, y_test_2d, seq_length=STAGE1_SEQUENCE_LENGTH)\n",
    "print(f'Test: {X_test_3d.shape}')\n",
    "\n",
    "print(f'\\n✓ All sets converted to 3D format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply Stage 1 Model with Correct Input Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'Applying Stage 1 model to filtered datasets...')\n",
    "\n",
    "# === Train Set ===\n",
    "print(f'\\n=== TRAIN SET ===')\n",
    "print(f'Input shape: {X_train_3d.shape}')\n",
    "stage1_probs_train = stage1_model.predict(X_train_3d, verbose=0)\n",
    "stage1_preds_train = (stage1_probs_train[:, 1] > 0.5).astype(int)\n",
    "signal_mask = stage1_preds_train == 1\n",
    "print(f'Signals detected: {signal_mask.sum()} / {len(X_train_3d)} ({100*signal_mask.sum()/len(X_train_3d):.2f}%)')\n",
    "\n",
    "# Filter to Stage 2 data\n",
    "X_stage2_train_3d = X_train_3d[signal_mask]\n",
    "y_stage2_train = y_train_3d[signal_mask]\n",
    "\n",
    "# Convert back to 2D for Stage 2 model (use last bar features)\n",
    "X_stage2_train = X_stage2_train_3d[:, -1, :]  # Use last timestep only\n",
    "\n",
    "# Remove invalid labels (0 = no zigzag)\n",
    "valid_mask = y_stage2_train > 0\n",
    "X_stage2_train = X_stage2_train[valid_mask]\n",
    "y_stage2_train = y_stage2_train[valid_mask]\n",
    "print(f'Valid Stage 2 samples (train): {len(X_stage2_train)}')\n",
    "\n",
    "# === Validation Set ===\n",
    "print(f'\\n=== VALIDATION SET ===')\n",
    "print(f'Input shape: {X_val_3d.shape}')\n",
    "stage1_probs_val = stage1_model.predict(X_val_3d, verbose=0)\n",
    "stage1_preds_val = (stage1_probs_val[:, 1] > 0.5).astype(int)\n",
    "signal_mask_val = stage1_preds_val == 1\n",
    "print(f'Signals detected: {signal_mask_val.sum()} / {len(X_val_3d)} ({100*signal_mask_val.sum()/len(X_val_3d):.2f}%)')\n",
    "\n",
    "X_stage2_val_3d = X_val_3d[signal_mask_val]\n",
    "y_stage2_val = y_val_3d[signal_mask_val]\n",
    "X_stage2_val = X_stage2_val_3d[:, -1, :]\n",
    "valid_mask_val = y_stage2_val > 0\n",
    "X_stage2_val = X_stage2_val[valid_mask_val]\n",
    "y_stage2_val = y_stage2_val[valid_mask_val]\n",
    "print(f'Valid Stage 2 samples (val): {len(X_stage2_val)}')\n",
    "\n",
    "# === Test Set ===\n",
    "print(f'\\n=== TEST SET ===')\n",
    "print(f'Input shape: {X_test_3d.shape}')\n",
    "stage1_probs_test = stage1_model.predict(X_test_3d, verbose=0)\n",
    "stage1_preds_test = (stage1_probs_test[:, 1] > 0.5).astype(int)\n",
    "signal_mask_test = stage1_preds_test == 1\n",
    "print(f'Signals detected: {signal_mask_test.sum()} / {len(X_test_3d)} ({100*signal_mask_test.sum()/len(X_test_3d):.2f}%)')\n",
    "\n",
    "X_stage2_test_3d = X_test_3d[signal_mask_test]\n",
    "y_stage2_test = y_test_3d[signal_mask_test]\n",
    "X_stage2_test = X_stage2_test_3d[:, -1, :]\n",
    "valid_mask_test = y_stage2_test > 0\n",
    "X_stage2_test = X_stage2_test[valid_mask_test]\n",
    "y_stage2_test = y_stage2_test[valid_mask_test]\n",
    "print(f'Valid Stage 2 samples (test): {len(X_stage2_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Stage 2 Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'Saving Stage 2 data to {STAGE2_DATA_DIR}...')\n",
    "\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_train, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_train, f)\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_val.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_val, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_val, f)\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_test, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_test, f)\n",
    "\n",
    "print('✓ All data saved')\n",
    "\n",
    "# List saved files\n",
    "print(f'\\nSaved files:')\n",
    "for file in sorted(STAGE2_DATA_DIR.glob('*.pkl')):\n",
    "    size = file.stat().st_size / 1024\n",
    "    print(f'  - {file.name} ({size:.0f} KB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train Stage 2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.stage2_trainer import Stage2Trainer\n",
    "\n",
    "print(f'Initializing Stage 2 Trainer...')\n",
    "trainer = Stage2Trainer(model_dir=str(STAGE2_MODEL_DIR))\n",
    "\n",
    "print(f'\\nTraining Stage 2 model...')\n",
    "print(f'Train samples: {len(X_stage2_train)}')\n",
    "print(f'Val samples: {len(X_stage2_val)}')\n",
    "print(f'Test samples: {len(X_stage2_test)}')\n",
    "print(f'Input shape: {X_stage2_train.shape}')\n",
    "\n",
    "train_results = trainer.train(\n",
    "    X_stage2_train, y_stage2_train,\n",
    "    X_stage2_val, y_stage2_val,\n",
    "    normalize=True,\n",
    "    cv_folds=5,\n",
    "    save_model=True\n",
    ")\n",
    "\n",
    "print(f'\\nTraining Results:')\n",
    "for key, value in train_results.items():\n",
    "    print(f'  {key}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'Evaluating on test set...')\n",
    "test_metrics = trainer.evaluate(X_stage2_test, y_stage2_test)\n",
    "\n",
    "print(f'\\nTest Metrics:')\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f'  {metric}: {value:.4f}')\n",
    "\n",
    "# Per-class performance\n",
    "print(f'\\nTest Predictions:')\n",
    "y_pred = trainer.predict(X_stage2_test)\n",
    "print(f'Unique labels in predictions: {np.unique(y_pred)}')\n",
    "print(f'Unique labels in ground truth: {np.unique(y_stage2_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'Running 5-fold cross-validation...')\n",
    "cv_results = trainer.cross_validate(\n",
    "    np.vstack([X_stage2_train, X_stage2_val]),\n",
    "    np.hstack([y_stage2_train, y_stage2_val]),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "print(f'\\nCross-Validation Results:')\n",
    "print(f'  Mean Accuracy: {cv_results[\"mean_accuracy\"]:.4f}')\n",
    "print(f'  Std Accuracy: {cv_results[\"std_accuracy\"]:.4f}')\n",
    "print(f'  Min Accuracy: {cv_results[\"min_accuracy\"]:.4f}')\n",
    "print(f'  Max Accuracy: {cv_results[\"max_accuracy\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'\\n' + '='*80)\n",
    "print(f'STAGE 2 TRAINING SUMMARY - {SYMBOL} {TIMEFRAME}')\n",
    "print(f'='*80)\n",
    "\n",
    "print(f'Stage 1 Architecture:')\n",
    "print(f'  Input shape: ({STAGE1_SEQUENCE_LENGTH}, {STAGE1_NUM_FEATURES})')\n",
    "print(f'  Type: 3D time series (LSTM/CNN)')\n",
    "\n",
    "print(f'\\nData Preparation:')\n",
    "print(f'  Original K-bars: {len(df):,}')\n",
    "print(f'  3D Sequences: {len(X_train_3d) + len(X_val_3d) + len(X_test_3d):,}')\n",
    "print(f'  Stage 1 Signals: {signal_mask.sum() + signal_mask_val.sum() + signal_mask_test.sum():,}')\n",
    "print(f'  Stage 2 Valid: {len(X_stage2_train) + len(X_stage2_val) + len(X_stage2_test):,}')\n",
    "\n",
    "print(f'Train/Val/Test Split:')\n",
    "print(f'  Train: {len(X_stage2_train):,}')\n",
    "print(f'  Val: {len(X_stage2_val):,}')\n",
    "print(f'  Test: {len(X_stage2_test):,}')\n",
    "\n",
    "print(f'Model Performance:')\n",
    "print(f'  Train Accuracy: {train_results[\"train_accuracy\"]:.4f}')\n",
    "print(f'  Val Accuracy: {train_results[\"val_accuracy\"]:.4f}')\n",
    "print(f'  Test Accuracy: {test_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'  Test F1-Score: {test_metrics[\"f1_score\"]:.4f}')\n",
    "\n",
    "print(f'Cross-Validation:')\n",
    "print(f'  Mean Accuracy: {cv_results[\"mean_accuracy\"]:.4f} +/- {cv_results[\"std_accuracy\"]:.4f}')\n",
    "\n",
    "print(f'Models Saved:')\n",
    "print(f'  Location: {STAGE2_MODEL_DIR}')\n",
    "for file in sorted(STAGE2_MODEL_DIR.glob('*')):\n",
    "    if file.is_file():\n",
    "        size = file.stat().st_size / 1024 / 1024\n",
    "        print(f'    - {file.name} ({size:.2f} MB)')\n",
    "\n",
    "print(f'='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
