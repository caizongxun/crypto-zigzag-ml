{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 Training on Google Colab\n",
    "\n",
    "ÂÆåÊï¥ÁöÑ Colab ÈÅ†Á®ãË®ìÁ∑¥ÊµÅÁ®ã\n",
    "\n",
    "Ë≤ªÁî®Ôºö**ÂÆåÂÖ®ÂÖçË≤ª** (GPU Âä†ÈÄü)\n",
    "ËÄóÊôÇÔºöÁ¥Ñ **5-8 ÂàÜÈêò**\n",
    "\n",
    "## ‰ΩøÁî®Ë™™Êòé\n",
    "\n",
    "1. ÈªûÊìä‰∏äÊñπ **Open in Colab** ÊåâÈçµ\n",
    "2. ÊàñË§áË£Ω Notebook ÈÄ£ÁµêÂà∞ Colab\n",
    "3. ‰æùÂ∫èÈÅãË°åÊâÄÊúâ Cell\n",
    "4. ÁµêÊûúËá™Âãï‰øùÂ≠òÂà∞ Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Áí∞Â¢ÉË®≠ÁΩÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ê™¢Êü• Colab Áí∞Â¢É\n",
    "import sys\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('‚úì Running on Google Colab')\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print('‚ö† Not running on Google Colab (local execution)')\n",
    "\n",
    "# Ê™¢Êü• GPU\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'‚úì GPU Available: {[gpu.name for gpu in gpus]}')\n",
    "else:\n",
    "    print('‚ö† No GPU detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ÂÆâË£ùÂøÖË¶ÅÁöÑÂ•ó‰ª∂\n",
    "print('Installing dependencies...')\n",
    "\n",
    "!pip install -q lightgbm huggingface-hub scikit-learn pandas numpy tensorflow\n",
    "\n",
    "print('‚úì All dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ÊéõËºâ Google DriveÔºàÂèØÈÅ∏Ôºâ\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print('‚úì Google Drive mounted at /content/drive')\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "else:\n",
    "    DRIVE_ROOT = '.'\n",
    "    print('‚ö† Not using Google Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ÂÖãÈöÜ GitHub ÂÄâÂ∫´\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = 'https://github.com/caizongxun/crypto-zigzag-ml.git'\n",
    "REPO_DIR = '/content/crypto-zigzag-ml'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(f'Cloning repository from {REPO_URL}...')\n",
    "    os.system(f'git clone {REPO_URL} {REPO_DIR}')\n",
    "    print('‚úì Repository cloned')\n",
    "else:\n",
    "    print(f'‚úì Repository already exists at {REPO_DIR}')\n",
    "    print('  Pulling latest changes...')\n",
    "    os.chdir(REPO_DIR)\n",
    "    os.system('git pull')\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f'Current directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 Ë®ìÁ∑¥Ôºà‰∏ªË¶ÅÊµÅÁ®ãÔºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ÈÖçÁΩÆ\n",
    "SYMBOL = 'BTCUSDT'\n",
    "TIMEFRAME = '15m'\n",
    "SYMBOL_SHORT = 'BTC'\n",
    "HF_DATASET_ID = 'zongowo111/v2-crypto-ohlcv-data'\n",
    "HF_MODEL_PATH = f'v1_model/{SYMBOL}/{TIMEFRAME}'\n",
    "HF_DATA_PATH = f'klines/{SYMBOL}/{SYMBOL_SHORT}_{TIMEFRAME}.parquet'\n",
    "\n",
    "# Stage 1 Model Architecture Parameters\n",
    "STAGE1_SEQUENCE_LENGTH = 10\n",
    "STAGE1_NUM_FEATURES = 20\n",
    "\n",
    "# Êú¨Âú∞Ë∑ØÂæë\n",
    "DATA_DIR = Path('data')\n",
    "MODEL_DIR = Path('models')\n",
    "STAGE1_MODEL_DIR = MODEL_DIR / 'stage1' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "STAGE2_DATA_DIR = DATA_DIR / 'stage2' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "STAGE2_MODEL_DIR = MODEL_DIR / 'stage2' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "\n",
    "# Âª∫Á´ãÁõÆÈåÑ\n",
    "STAGE1_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STAGE2_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STAGE2_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Symbol: {SYMBOL}')\n",
    "print(f'Timeframe: {TIMEFRAME}')\n",
    "print(f'Stage 1 Input Shape: ({STAGE1_SEQUENCE_LENGTH}, {STAGE1_NUM_FEATURES})')\n",
    "print(f'Stage 1 Model Dir: {STAGE1_MODEL_DIR}')\n",
    "print(f'Stage 2 Model Dir: {STAGE2_MODEL_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 1: Âæû HuggingFace ‰∏ãËºâ Stage 1 Ê®°Âûã\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f'[1/7] Downloading Stage 1 model from HuggingFace...')\n",
    "print(f'Dataset: {HF_DATASET_ID}')\n",
    "print(f'Path: {HF_MODEL_PATH}')\n",
    "\n",
    "# ‰∏ãËºâ classification.h5\n",
    "try:\n",
    "    classification_path = hf_hub_download(\n",
    "        repo_id=HF_DATASET_ID,\n",
    "        filename=f'{HF_MODEL_PATH}/classification.h5',\n",
    "        repo_type='dataset',\n",
    "        cache_dir=str(STAGE1_MODEL_DIR)\n",
    "    )\n",
    "    print(f'‚úì Downloaded classification.h5')\n",
    "except Exception as e:\n",
    "    print(f'‚úó Error: {e}')\n",
    "    raise\n",
    "\n",
    "# Âä†ËºâÊ®°Âûã\n",
    "classification_files = list(STAGE1_MODEL_DIR.rglob('classification.h5'))\n",
    "stage1_model_path = classification_files[0]\n",
    "stage1_model = keras.models.load_model(str(stage1_model_path))\n",
    "\n",
    "# Get input shape\n",
    "input_shape = stage1_model.input_shape\n",
    "if len(input_shape) == 3:\n",
    "    STAGE1_SEQUENCE_LENGTH = input_shape[1]\n",
    "    STAGE1_NUM_FEATURES = input_shape[2]\n",
    "    print(f'‚úì Stage 1 model loaded')\n",
    "    print(f'  Input shape: {input_shape}')\n",
    "else:\n",
    "    raise ValueError(f'Expected 3D input, got {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 2: Âæû HuggingFace ‰∏ãËºâË®ìÁ∑¥Êï∏Êìö\n",
    "print(f'\\n[2/7] Downloading training data from HuggingFace...')\n",
    "print(f'File: {HF_DATA_PATH}')\n",
    "\n",
    "try:\n",
    "    data_file = hf_hub_download(\n",
    "        repo_id=HF_DATASET_ID,\n",
    "        filename=HF_DATA_PATH,\n",
    "        repo_type='dataset',\n",
    "        cache_dir=str(DATA_DIR)\n",
    "    )\n",
    "    print(f'‚úì Downloaded data')\n",
    "    data_file = Path(data_file)\n",
    "except Exception as e:\n",
    "    print(f'‚úó Error downloading: {e}')\n",
    "    raise\n",
    "\n",
    "# Âä†ËºâÊï∏Êìö\n",
    "df = pd.read_parquet(data_file)\n",
    "print(f'‚úì Data loaded: {df.shape[0]:,} rows, {df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 3: ÁâπÂæµÂ∑•Á®ãÂíå ZigZag Ê®ôÁ±§\n",
    "print(f'\\n[3/7] Feature engineering...')\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "from src.zigzag_indicator import ZigZagIndicator\n",
    "from src.features import FeatureEngineer\n",
    "from src.utils import time_series_split\n",
    "\n",
    "# ÊáâÁî® ZigZag\n",
    "zigzag = ZigZagIndicator(depth=12, deviation=5, backstep=2)\n",
    "df = zigzag.label_kbars(df)\n",
    "print(f'‚úì ZigZag labels applied')\n",
    "\n",
    "# Ë®àÁÆóÊäÄË°ìÊåáÊ®ô\n",
    "fe = FeatureEngineer(lookback_periods=[5, 10, 20, 50, 200])\n",
    "df = fe.calculate_all_features(df)\n",
    "\n",
    "# Áç≤ÂèñÁâπÂæµÂàó\n",
    "feature_cols = fe.get_feature_columns(df)\n",
    "if 'symbol' in feature_cols:\n",
    "    feature_cols.remove('symbol')\n",
    "\n",
    "# ËôïÁêÜÁº∫Â§±ÂÄº\n",
    "df[feature_cols] = df[feature_cols].fillna(method='ffill').fillna(0)\n",
    "print(f'‚úì Features calculated: {len(feature_cols)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 4: Create 3D Time Series Features\n",
    "print(f'\\n[4/7] Data splitting and Stage 1 filtering...')\n",
    "\n",
    "def create_3d_sequences(X, y, seq_length=10):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Split data\n",
    "train_df, val_df, test_df = time_series_split(df, train_ratio=0.7, validation_ratio=0.15)\n",
    "\n",
    "# === Train Set ===\n",
    "X_train_2d = train_df[feature_cols].values\n",
    "y_train_2d = train_df['zigzag_label'].values\n",
    "X_train_3d, y_train_3d = create_3d_sequences(X_train_2d, y_train_2d, seq_length=STAGE1_SEQUENCE_LENGTH)\n",
    "print(f'  Train 3D shape: {X_train_3d.shape}')\n",
    "\n",
    "# === Validation Set ===\n",
    "X_val_2d = val_df[feature_cols].values\n",
    "y_val_2d = val_df['zigzag_label'].values\n",
    "X_val_3d, y_val_3d = create_3d_sequences(X_val_2d, y_val_2d, seq_length=STAGE1_SEQUENCE_LENGTH)\n",
    "print(f'  Val 3D shape: {X_val_3d.shape}')\n",
    "\n",
    "# === Test Set ===\n",
    "X_test_2d = test_df[feature_cols].values\n",
    "y_test_2d = test_df['zigzag_label'].values\n",
    "X_test_3d, y_test_3d = create_3d_sequences(X_test_2d, y_test_2d, seq_length=STAGE1_SEQUENCE_LENGTH)\n",
    "print(f'  Test 3D shape: {X_test_3d.shape}')\n",
    "\n",
    "print(f'‚úì All sets converted to 3D format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5: Apply Stage 1 Model\n",
    "print(f'\\nApplying Stage 1 model to filtered datasets...')\n",
    "\n",
    "# === Train Set ===\n",
    "stage1_probs_train = stage1_model.predict(X_train_3d, verbose=0)\n",
    "stage1_preds_train = (stage1_probs_train[:, 1] > 0.5).astype(int)\n",
    "signal_mask = stage1_preds_train == 1\n",
    "X_stage2_train_3d = X_train_3d[signal_mask]\n",
    "y_stage2_train = y_train_3d[signal_mask]\n",
    "X_stage2_train = X_stage2_train_3d[:, -1, :]\n",
    "valid_mask = y_stage2_train > 0\n",
    "X_stage2_train = X_stage2_train[valid_mask]\n",
    "y_stage2_train = y_stage2_train[valid_mask]\n",
    "print(f'  Train: {len(X_stage2_train):,} samples')\n",
    "\n",
    "# === Validation Set ===\n",
    "stage1_probs_val = stage1_model.predict(X_val_3d, verbose=0)\n",
    "stage1_preds_val = (stage1_probs_val[:, 1] > 0.5).astype(int)\n",
    "signal_mask_val = stage1_preds_val == 1\n",
    "X_stage2_val_3d = X_val_3d[signal_mask_val]\n",
    "y_stage2_val = y_val_3d[signal_mask_val]\n",
    "X_stage2_val = X_stage2_val_3d[:, -1, :]\n",
    "valid_mask_val = y_stage2_val > 0\n",
    "X_stage2_val = X_stage2_val[valid_mask_val]\n",
    "y_stage2_val = y_stage2_val[valid_mask_val]\n",
    "print(f'  Val: {len(X_stage2_val):,} samples')\n",
    "\n",
    "# === Test Set ===\n",
    "stage1_probs_test = stage1_model.predict(X_test_3d, verbose=0)\n",
    "stage1_preds_test = (stage1_probs_test[:, 1] > 0.5).astype(int)\n",
    "signal_mask_test = stage1_preds_test == 1\n",
    "X_stage2_test_3d = X_test_3d[signal_mask_test]\n",
    "y_stage2_test = y_test_3d[signal_mask_test]\n",
    "X_stage2_test = X_stage2_test_3d[:, -1, :]\n",
    "valid_mask_test = y_stage2_test > 0\n",
    "X_stage2_test = X_stage2_test[valid_mask_test]\n",
    "y_stage2_test = y_stage2_test[valid_mask_test]\n",
    "print(f'  Test: {len(X_stage2_test):,} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 6: ‰øùÂ≠ò Stage 2 Ë®ìÁ∑¥Êï∏Êìö\n",
    "print(f'\\n[5/7] Saving Stage 2 training data...')\n",
    "\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_train, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_train, f)\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_val.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_val, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_val, f)\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_test, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_test, f)\n",
    "\n",
    "print(f'‚úì Data saved to {STAGE2_DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 7: Ë®ìÁ∑¥ Stage 2 Ê®°Âûã\n",
    "print(f'\\n[6/7] Training Stage 2 model...')\n",
    "\n",
    "from src.stage2_trainer import Stage2Trainer\n",
    "\n",
    "trainer = Stage2Trainer(model_dir=str(STAGE2_MODEL_DIR))\n",
    "\n",
    "train_results = trainer.train(\n",
    "    X_stage2_train, y_stage2_train,\n",
    "    X_stage2_val, y_stage2_val,\n",
    "    normalize=True,\n",
    "    cv_folds=5,\n",
    "    save_model=True\n",
    ")\n",
    "\n",
    "print(f'‚úì Model trained')\n",
    "print(f'  Train Accuracy: {train_results[\"train_accuracy\"]:.4f}')\n",
    "print(f'  Val Accuracy: {train_results[\"val_accuracy\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 8: Ë©ï‰º∞Âíå‰∫§ÂèâÈ©óË≠â\n",
    "print(f'\\n[7/7] Evaluation and cross-validation...')\n",
    "\n",
    "test_metrics = trainer.evaluate(X_stage2_test, y_stage2_test)\n",
    "\n",
    "cv_results = trainer.cross_validate(\n",
    "    np.vstack([X_stage2_train, X_stage2_val]),\n",
    "    np.hstack([y_stage2_train, y_stage2_val]),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "print(f'‚úì Evaluation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÁµêÊûúÁ∏ΩÁµê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'\\n' + '='*80)\n",
    "print(f'STAGE 2 TRAINING COMPLETE - {SYMBOL} {TIMEFRAME}')\n",
    "print(f'='*80)\n",
    "\n",
    "print(f'\\nüìä DATA STATISTICS:')\n",
    "print(f'  Original K-bars: {len(df):,}')\n",
    "print(f'  Stage 1 Signals: {signal_mask.sum() + signal_mask_val.sum() + signal_mask_test.sum():,}')\n",
    "print(f'  Stage 2 Valid Samples: {len(X_stage2_train) + len(X_stage2_val) + len(X_stage2_test):,}')\n",
    "\n",
    "print(f'\\nüìà TRAIN/VAL/TEST SPLIT:')\n",
    "print(f'  Train: {len(X_stage2_train):,}')\n",
    "print(f'  Val: {len(X_stage2_val):,}')\n",
    "print(f'  Test: {len(X_stage2_test):,}')\n",
    "\n",
    "print(f'\\nüéØ MODEL PERFORMANCE:')\n",
    "print(f'  Train Accuracy: {train_results[\"train_accuracy\"]:.4f}')\n",
    "print(f'  Val Accuracy: {train_results[\"val_accuracy\"]:.4f}')\n",
    "print(f'  Test Accuracy: {test_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'  Test F1-Score: {test_metrics[\"f1_score\"]:.4f}')\n",
    "\n",
    "print(f'\\n‚úÖ CROSS-VALIDATION:')\n",
    "print(f'  Mean Accuracy: {cv_results[\"mean_accuracy\"]:.4f}')\n",
    "print(f'  Std Accuracy: {cv_results[\"std_accuracy\"]:.4f}')\n",
    "\n",
    "print(f'\\nüíæ MODELS SAVED:')\n",
    "for file in sorted(STAGE2_MODEL_DIR.glob('*')):\n",
    "    if file.is_file():\n",
    "        size = file.stat().st_size / 1024 / 1024\n",
    "        print(f'  {file.name} ({size:.2f} MB)')\n",
    "\n",
    "print(f'\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‰øùÂ≠òÁµêÊûúÂà∞ Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    from google.colab import drive\n",
    "    \n",
    "    print('Saving results to Google Drive...')\n",
    "    \n",
    "    # Âª∫Á´ã Colab ÁµêÊûúÁõÆÈåÑ\n",
    "    colab_results = Path(DRIVE_ROOT) / 'Colab Results' / 'Stage2' / f'{SYMBOL}_{TIMEFRAME}'\n",
    "    colab_results.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ë§áË£ΩÊ®°ÂûãÊ™îÊ°à\n",
    "    for model_file in STAGE2_MODEL_DIR.glob('*'):\n",
    "        if model_file.is_file():\n",
    "            shutil.copy(model_file, colab_results / model_file.name)\n",
    "            print(f'  ‚úì Copied {model_file.name}')\n",
    "    \n",
    "    # Ë§áË£ΩË®ìÁ∑¥Êï∏Êìö\n",
    "    data_subdir = colab_results / 'training_data'\n",
    "    data_subdir.mkdir(parents=True, exist_ok=True)\n",
    "    for data_file in STAGE2_DATA_DIR.glob('*.pkl'):\n",
    "        shutil.copy(data_file, data_subdir / data_file.name)\n",
    "        print(f'  ‚úì Copied {data_file.name}')\n",
    "    \n",
    "    print(f'\\n‚úì Results saved to: {colab_results}')\n",
    "else:\n",
    "    print('Not running on Colab - skipping Google Drive save')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "colab": {
   "name": "Stage 2 Training - Colab",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
