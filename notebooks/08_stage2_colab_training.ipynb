{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 Training on Google Colab\n",
    "\n",
    "å®Œæ•´çš„ Colab é ç¨‹è¨“ç·´æµç¨‹\n",
    "\n",
    "è²»ç”¨ï¼š**å®Œå…¨å…è²»** (GPU åŠ é€Ÿ)\n",
    "è€—æ™‚ï¼šç´„ **5-8 åˆ†é˜**\n",
    "\n",
    "## ä½¿ç”¨èªªæ˜\n",
    "\n",
    "1. é»æ“Šä¸Šæ–¹ **Open in Colab** æŒ‰éˆ•\n",
    "2. æˆ–è¤‡è£½ Notebook é€£çµåˆ° Colab\n",
    "3. ä¾åºé‹è¡Œæ‰€æœ‰ Cell\n",
    "4. çµæœè‡ªå‹•ä¿å­˜åˆ° Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# æª¢æŸ¥ Colab ç’°å¢ƒ\n",
    "import sys\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('âœ“ Running on Google Colab')\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print('âš  Not running on Google Colab (local execution)')\n",
    "\n",
    "# æª¢æŸ¥ GPU\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'âœ“ GPU Available: {[gpu.name for gpu in gpus]}')\n",
    "else:\n",
    "    print('âš  No GPU detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# å®‰è£å¿…è¦çš„å¥—ä»¶\n",
    "print('Installing dependencies...')\n",
    "\n",
    "!pip install -q lightgbm huggingface-hub scikit-learn pandas numpy tensorflow\n",
    "\n",
    "print('âœ“ All dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# æ›è¼‰ Google Driveï¼ˆå¯é¸ï¼‰\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print('âœ“ Google Drive mounted at /content/drive')\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "else:\n",
    "    DRIVE_ROOT = '.'\n",
    "    print('âš  Not using Google Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# å…‹éš† GitHub å€‰åº«\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = 'https://github.com/caizongxun/crypto-zigzag-ml.git'\n",
    "REPO_DIR = '/content/crypto-zigzag-ml'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(f'Cloning repository from {REPO_URL}...')\n",
    "    os.system(f'git clone {REPO_URL} {REPO_DIR}')\n",
    "    print('âœ“ Repository cloned')\n",
    "else:\n",
    "    print(f'âœ“ Repository already exists at {REPO_DIR}')\n",
    "    print('  Pulling latest changes...')\n",
    "    os.chdir(REPO_DIR)\n",
    "    os.system('git pull')\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f'Current directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 è¨“ç·´ï¼ˆä¸»è¦æµç¨‹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# é…ç½®\n",
    "SYMBOL = 'BTCUSDT'\n",
    "TIMEFRAME = '15m'\n",
    "HF_DATASET_ID = 'zongowo111/v2-crypto-ohlcv-data'\n",
    "HF_MODEL_PATH = f'v1_model/{SYMBOL}/{TIMEFRAME}'\n",
    "HF_DATA_PATH = f'klines/{SYMBOL}/{SYMBOL.split(\"/\")[0]}_15m.parquet'\n",
    "\n",
    "# æœ¬åœ°è·¯å¾‘\n",
    "DATA_DIR = Path('data')\n",
    "MODEL_DIR = Path('models')\n",
    "STAGE1_MODEL_DIR = MODEL_DIR / 'stage1' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "STAGE2_DATA_DIR = DATA_DIR / 'stage2' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "STAGE2_MODEL_DIR = MODEL_DIR / 'stage2' / f'{SYMBOL.lower()}_{TIMEFRAME}'\n",
    "\n",
    "# å»ºç«‹ç›®éŒ„\n",
    "STAGE1_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STAGE2_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STAGE2_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Symbol: {SYMBOL}')\n",
    "print(f'Timeframe: {TIMEFRAME}')\n",
    "print(f'Stage 1 Model Dir: {STAGE1_MODEL_DIR}')\n",
    "print(f'Stage 2 Model Dir: {STAGE2_MODEL_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 1: å¾ HuggingFace ä¸‹è¼‰ Stage 1 æ¨¡å‹\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f'[1/7] Downloading Stage 1 model from HuggingFace...')\n",
    "print(f'Dataset: {HF_DATASET_ID}')\n",
    "print(f'Path: {HF_MODEL_PATH}')\n",
    "\n",
    "# ä¸‹è¼‰ classification.h5\n",
    "try:\n",
    "    classification_path = hf_hub_download(\n",
    "        repo_id=HF_DATASET_ID,\n",
    "        filename=f'{HF_MODEL_PATH}/classification.h5',\n",
    "        repo_type='dataset',\n",
    "        cache_dir=str(STAGE1_MODEL_DIR)\n",
    "    )\n",
    "    print(f'âœ“ Downloaded classification.h5')\n",
    "except Exception as e:\n",
    "    print(f'âœ— Error: {e}')\n",
    "    raise\n",
    "\n",
    "# åŠ è¼‰æ¨¡å‹\n",
    "classification_files = list(STAGE1_MODEL_DIR.rglob('classification.h5'))\n",
    "stage1_model_path = classification_files[0]\n",
    "stage1_model = keras.models.load_model(str(stage1_model_path))\n",
    "print(f'âœ“ Stage 1 model loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 2: å¾ HuggingFace ä¸‹è¼‰è¨“ç·´æ•¸æ“š\n",
    "print(f'\\n[2/7] Downloading training data from HuggingFace...')\n",
    "\n",
    "try:\n",
    "    data_file = hf_hub_download(\n",
    "        repo_id=HF_DATASET_ID,\n",
    "        filename=HF_DATA_PATH,\n",
    "        repo_type='dataset',\n",
    "        cache_dir=str(DATA_DIR)\n",
    "    )\n",
    "    print(f'âœ“ Downloaded data')\n",
    "    data_file = Path(data_file)\n",
    "except Exception as e:\n",
    "    print(f'âœ— Error: {e}')\n",
    "    raise\n",
    "\n",
    "# åŠ è¼‰æ•¸æ“š\n",
    "df = pd.read_parquet(data_file)\n",
    "print(f'âœ“ Data loaded: {df.shape[0]:,} rows, {df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 3: ç‰¹å¾µå·¥ç¨‹å’Œ ZigZag æ¨™ç±¤\n",
    "print(f'\\n[3/7] Feature engineering...')\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "from src.zigzag_indicator import ZigZagIndicator\n",
    "from src.features import FeatureEngineer\n",
    "from src.utils import time_series_split\n",
    "\n",
    "# æ‡‰ç”¨ ZigZag\n",
    "zigzag = ZigZagIndicator(depth=12, deviation=5, backstep=2)\n",
    "df = zigzag.label_kbars(df)\n",
    "print(f'âœ“ ZigZag labels applied')\n",
    "\n",
    "# è¨ˆç®—æŠ€è¡“æŒ‡æ¨™\n",
    "fe = FeatureEngineer(lookback_periods=[5, 10, 20, 50, 200])\n",
    "df = fe.calculate_all_features(df)\n",
    "\n",
    "# ç²å–ç‰¹å¾µåˆ—\n",
    "feature_cols = fe.get_feature_columns(df)\n",
    "if 'symbol' in feature_cols:\n",
    "    feature_cols.remove('symbol')\n",
    "\n",
    "# è™•ç†ç¼ºå¤±å€¼\n",
    "df[feature_cols] = df[feature_cols].fillna(method='ffill').fillna(0)\n",
    "print(f'âœ“ Features calculated: {len(feature_cols)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 4: è¨“ç·´-é©—è­‰-æ¸¬è©¦ Split å’Œ Stage 1 ç¯©é¸\n",
    "print(f'\\n[4/7] Data splitting and Stage 1 filtering...')\n",
    "\n",
    "train_df, val_df, test_df = time_series_split(df, train_ratio=0.7, validation_ratio=0.15)\n",
    "\n",
    "# è¨“ç·´é›†\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['zigzag_label'].values\n",
    "stage1_probs_train = stage1_model.predict(X_train, verbose=0)\n",
    "stage1_preds_train = (stage1_probs_train[:, 1] > 0.5).astype(int)\n",
    "signal_mask = stage1_preds_train == 1\n",
    "X_stage2_train = X_train[signal_mask]\n",
    "y_stage2_train = y_train[signal_mask]\n",
    "valid_mask = y_stage2_train > 0\n",
    "X_stage2_train = X_stage2_train[valid_mask]\n",
    "y_stage2_train = y_stage2_train[valid_mask]\n",
    "print(f'  Train: {len(X_stage2_train):,} samples')\n",
    "\n",
    "# é©—è­‰é›†\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['zigzag_label'].values\n",
    "stage1_probs_val = stage1_model.predict(X_val, verbose=0)\n",
    "stage1_preds_val = (stage1_probs_val[:, 1] > 0.5).astype(int)\n",
    "signal_mask_val = stage1_preds_val == 1\n",
    "X_stage2_val = X_val[signal_mask_val]\n",
    "y_stage2_val = y_val[signal_mask_val]\n",
    "valid_mask_val = y_stage2_val > 0\n",
    "X_stage2_val = X_stage2_val[valid_mask_val]\n",
    "y_stage2_val = y_stage2_val[valid_mask_val]\n",
    "print(f'  Val: {len(X_stage2_val):,} samples')\n",
    "\n",
    "# æ¸¬è©¦é›†\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['zigzag_label'].values\n",
    "stage1_probs_test = stage1_model.predict(X_test, verbose=0)\n",
    "stage1_preds_test = (stage1_probs_test[:, 1] > 0.5).astype(int)\n",
    "signal_mask_test = stage1_preds_test == 1\n",
    "X_stage2_test = X_test[signal_mask_test]\n",
    "y_stage2_test = y_test[signal_mask_test]\n",
    "valid_mask_test = y_stage2_test > 0\n",
    "X_stage2_test = X_stage2_test[valid_mask_test]\n",
    "y_stage2_test = y_stage2_test[valid_mask_test]\n",
    "print(f'  Test: {len(X_stage2_test):,} samples')\n",
    "\n",
    "print(f'âœ“ Data split and filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5: ä¿å­˜ Stage 2 è¨“ç·´æ•¸æ“š\n",
    "print(f'\\n[5/7] Saving Stage 2 training data...')\n",
    "\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_train, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_train, f)\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_val.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_val, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_val, f)\n",
    "with open(STAGE2_DATA_DIR / 'X_stage2_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_stage2_test, f)\n",
    "with open(STAGE2_DATA_DIR / 'y_stage2_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_stage2_test, f)\n",
    "\n",
    "print(f'âœ“ Data saved to {STAGE2_DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 6: è¨“ç·´ Stage 2 æ¨¡å‹\n",
    "print(f'\\n[6/7] Training Stage 2 model...')\n",
    "\n",
    "from src.stage2_trainer import Stage2Trainer\n",
    "\n",
    "trainer = Stage2Trainer(model_dir=str(STAGE2_MODEL_DIR))\n",
    "\n",
    "train_results = trainer.train(\n",
    "    X_stage2_train, y_stage2_train,\n",
    "    X_stage2_val, y_stage2_val,\n",
    "    normalize=True,\n",
    "    cv_folds=5,\n",
    "    save_model=True\n",
    ")\n",
    "\n",
    "print(f'âœ“ Model trained')\n",
    "print(f'  Train Accuracy: {train_results[\"train_accuracy\"]:.4f}')\n",
    "print(f'  Val Accuracy: {train_results[\"val_accuracy\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 7: è©•ä¼°å’Œäº¤å‰é©—è­‰\n",
    "print(f'\\n[7/7] Evaluation and cross-validation...')\n",
    "\n",
    "test_metrics = trainer.evaluate(X_stage2_test, y_stage2_test)\n",
    "\n",
    "cv_results = trainer.cross_validate(\n",
    "    np.vstack([X_stage2_train, X_stage2_val]),\n",
    "    np.hstack([y_stage2_train, y_stage2_val]),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "print(f'âœ“ Evaluation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## çµæœç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f'\\n' + '='*80)\n",
    "print(f'STAGE 2 TRAINING COMPLETE - {SYMBOL} {TIMEFRAME}')\n",
    "print(f'='*80)\n",
    "\n",
    "print(f'\\nğŸ“Š DATA STATISTICS:')\n",
    "print(f'  Original K-bars: {len(df):,}')\n",
    "print(f'  Stage 1 Signals: {signal_mask.sum() + signal_mask_val.sum() + signal_mask_test.sum():,}')\n",
    "print(f'  Stage 2 Valid Samples: {len(X_stage2_train) + len(X_stage2_val) + len(X_stage2_test):,}')\n",
    "\n",
    "print(f'\\nğŸ“ˆ TRAIN/VAL/TEST SPLIT:')\n",
    "print(f'  Train: {len(X_stage2_train):,}')\n",
    "print(f'  Val: {len(X_stage2_val):,}')\n",
    "print(f'  Test: {len(X_stage2_test):,}')\n",
    "\n",
    "print(f'\\nğŸ¯ MODEL PERFORMANCE:')\n",
    "print(f'  Train Accuracy: {train_results[\"train_accuracy\"]:.4f}')\n",
    "print(f'  Val Accuracy: {train_results[\"val_accuracy\"]:.4f}')\n",
    "print(f'  Test Accuracy: {test_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'  Test F1-Score: {test_metrics[\"f1_score\"]:.4f}')\n",
    "\n",
    "print(f'\\nâœ… CROSS-VALIDATION:')\n",
    "print(f'  Mean Accuracy: {cv_results[\"mean_accuracy\"]:.4f}')\n",
    "print(f'  Std Accuracy: {cv_results[\"std_accuracy\"]:.4f}')\n",
    "print(f'  Min Accuracy: {cv_results[\"min_accuracy\"]:.4f}')\n",
    "print(f'  Max Accuracy: {cv_results[\"max_accuracy\"]:.4f}')\n",
    "\n",
    "print(f'\\nğŸ’¾ MODELS SAVED:')\n",
    "for file in sorted(STAGE2_MODEL_DIR.glob('*')):\n",
    "    if file.is_file():\n",
    "        size = file.stat().st_size / 1024 / 1024\n",
    "        print(f'  {file.name} ({size:.2f} MB)')\n",
    "\n",
    "print(f'\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¿å­˜çµæœåˆ° Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    from google.colab import drive\n",
    "    \n",
    "    print('Saving results to Google Drive...')\n",
    "    \n",
    "    # å»ºç«‹ Colab çµæœç›®éŒ„\n",
    "    colab_results = Path(DRIVE_ROOT) / 'Colab Results' / 'Stage2' / f'{SYMBOL}_{TIMEFRAME}'\n",
    "    colab_results.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # è¤‡è£½æ¨¡å‹æª”æ¡ˆ\n",
    "    for model_file in STAGE2_MODEL_DIR.glob('*'):\n",
    "        if model_file.is_file():\n",
    "            shutil.copy(model_file, colab_results / model_file.name)\n",
    "            print(f'  âœ“ Copied {model_file.name}')\n",
    "    \n",
    "    # è¤‡è£½è¨“ç·´æ•¸æ“š\n",
    "    data_subdir = colab_results / 'training_data'\n",
    "    data_subdir.mkdir(parents=True, exist_ok=True)\n",
    "    for data_file in STAGE2_DATA_DIR.glob('*.pkl'):\n",
    "        shutil.copy(data_file, data_subdir / data_file.name)\n",
    "        print(f'  âœ“ Copied {data_file.name}')\n",
    "    \n",
    "    print(f'\\nâœ“ Results saved to: {colab_results}')\n",
    "else:\n",
    "    print('Not running on Colab - skipping Google Drive save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‹ä¸€æ­¥ï¼šæ¨ç†å’Œéƒ¨ç½²\n",
    "\n",
    "è¨“ç·´å®Œæˆå¾Œï¼Œä½ å¯ä»¥ï¼š\n",
    "\n",
    "### 1. ä½¿ç”¨æ¨ç†ç®¡é“\n",
    "```python\n",
    "from src.stage2_inference import PipelineInference\n",
    "\n",
    "pipeline = PipelineInference(model_dir='models')\n",
    "pipeline.load_stage1_model(f'{SYMBOL.lower()}_{TIMEFRAME}')\n",
    "pipeline.load_stage2_model(f'{SYMBOL.lower()}_{TIMEFRAME}')\n",
    "\n",
    "# é æ¸¬\n",
    "pred = pipeline.predict(X_new)\n",
    "```\n",
    "\n",
    "### 2. è¨“ç·´å…¶ä»–å¹£ç¨®\n",
    "ä¿®æ”¹æœ¬ Notebook ä¸Šæ–¹çš„ `SYMBOL` å’Œ `TIMEFRAME` è®Šæ•¸ï¼Œç„¶å¾Œé‡æ–°é‹è¡Œ\n",
    "\n",
    "### 3. æ‰¹é‡è¨“ç·´æ‰€æœ‰ 22 å¹£ç¨®\n",
    "```bash\n",
    "python scripts/train_all_stage2.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "colab": {
   "name": "Stage 2 Training - Colab",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
