{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Crypto ZigZag ML - Batch Training All Symbols\n", "\n", "Train models for 22 symbols Ã— 2 timeframes (15m + 1h) on Google Colab\n", "\n", "This will generate:\n", "- 44 signal classification models (5-class)\n", "- 44 signal detection models (binary)\n", "- 44 signal type models (if enabled)\n", "- Total: 132 production models\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 1: Setup\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n", "print(f'GPU: {tf.config.list_physical_devices(\"GPU\")}')\n", "\n", "from google.colab import drive\n", "drive.mount('/content/drive')\n", "\n", "import os\n", "os.makedirs('/content/drive/MyDrive/crypto-zigzag-ml/models', exist_ok=True)\n", "os.makedirs('/content/drive/MyDrive/crypto-zigzag-ml/training_logs', exist_ok=True)\n", "print('Directories ready')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "project_root = '/content/drive/MyDrive/crypto-zigzag-ml'\n", "sys.path.insert(0, project_root)\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import pickle\n", "import json\n", "from datetime import datetime\n", "from tensorflow import keras\n", "from tensorflow.keras import layers\n", "\n", "from data.fetch_data import CryptoDataFetcher\n", "from src.zigzag_indicator import ZigZagIndicator\n", "from src.features import FeatureEngineer\n", "from src.utils import time_series_split\n", "\n", "print('All imports successful')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Define Symbols and Timeframes\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 22 symbols as specified\n", "SYMBOLS = [\n", "    'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'XRPUSDT', 'ADAUSDT',\n", "    'DOGEUSDT', 'MATICUSDT', 'LINKUSDT', 'LITUSDT', 'UNIUSDT',\n", "    'AVAXUSDT', 'SOLUUSDT', 'FTMUSDT', 'AAVEUSDT', 'CRVUSDT',\n", "    'MKRUSDT', 'SNXUSDT', 'COMPUSDT', 'LRCUSDT', 'GRTUSDT',\n", "    'ALGOUSDT', 'ATOMUSDT'\n", "]\n", "\n", "TIMEFRAMES = ['15m', '1h']\n", "\n", "print(f'Total configurations: {len(SYMBOLS)} symbols Ã— {len(TIMEFRAMES)} timeframes = {len(SYMBOLS) * len(TIMEFRAMES)} models')\n", "print(f'With 3 models per config: {len(SYMBOLS) * len(TIMEFRAMES) * 3} total models')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Model Builder Function\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_classification_model(input_shape):\n", "    \"\"\"Build 5-class signal classification model\"\"\"\n", "    inputs = layers.Input(shape=input_shape)\n", "    \n", "    x = layers.LSTM(256, return_sequences=True)(inputs)\n", "    x = layers.Dropout(0.3)(x)\n", "    x = layers.LSTM(128, return_sequences=False)(x)\n", "    x = layers.Dropout(0.3)(x)\n", "    x = layers.Dense(64, activation='relu')(x)\n", "    x = layers.Dropout(0.3)(x)\n", "    x = layers.Dense(32, activation='relu')(x)\n", "    x = layers.Dropout(0.2)(x)\n", "    \n", "    output = layers.Dense(5, activation='softmax')(x)\n", "    \n", "    model = keras.Model(inputs=inputs, outputs=output)\n", "    model.compile(\n", "        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n", "        loss='sparse_categorical_crossentropy',\n", "        metrics=['accuracy']\n", "    )\n", "    return model\n", "\n", "def build_detection_model(input_shape):\n", "    \"\"\"Build binary signal detection model\"\"\"\n", "    inputs = layers.Input(shape=input_shape)\n", "    \n", "    x = layers.LSTM(128, return_sequences=True)(inputs)\n", "    x = layers.Dropout(0.2)(x)\n", "    x = layers.LSTM(64, return_sequences=False)(x)\n", "    x = layers.Dropout(0.2)(x)\n", "    x = layers.Dense(32, activation='relu')(x)\n", "    x = layers.Dropout(0.2)(x)\n", "    \n", "    output = layers.Dense(1, activation='sigmoid')(x)\n", "    \n", "    model = keras.Model(inputs=inputs, outputs=output)\n", "    model.compile(\n", "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n", "        loss='binary_crossentropy',\n", "        metrics=['accuracy']\n", "    )\n", "    return model\n", "\n", "print('Model builders ready')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Training Function\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_symbol_timeframe(symbol, timeframe, fetcher, zigzag, fe):\n", "    \"\"\"Train models for one symbol+timeframe combination\"\"\"\n", "    \n", "    try:\n", "        print(f'\\n{'='*60}')\n", "        print(f'Training: {symbol} {timeframe}')\n", "        print(f'{'='*60}')\n", "        \n", "        # Fetch data\n", "        print(f'Fetching data...')\n", "        data = fetcher.fetch_symbol_timeframe(symbol, timeframe)\n", "        \n", "        if len(data) < 1000:\n", "            print(f'  âš  Insufficient data ({len(data)} bars), skipping')\n", "            return None\n", "        \n", "        print(f'  âœ“ {len(data)} bars fetched')\n", "        \n", "        # Apply ZigZag\n", "        print(f'Applying ZigZag...')\n", "        data = zigzag.label_kbars(data)\n", "        label_counts = data['zigzag_label'].value_counts()\n", "        print(f'  âœ“ Labels: {dict(label_counts)}'  )\n", "        \n", "        # Features\n", "        print(f'Engineering features...')\n", "        data = fe.calculate_all_features(data)\n", "        feature_cols = fe.get_feature_columns(data)\n", "        data[feature_cols] = data[feature_cols].fillna(method='ffill').fillna(0)\n", "        print(f'  âœ“ {len(feature_cols)} features')\n", "        \n", "        # Split\n", "        train_df, val_df, test_df = time_series_split(data, 0.7, 0.15)\n", "        \n", "        # Prepare\n", "        selected_features = feature_cols[:40]\n", "        X_train = train_df[selected_features].values.astype(np.float32)\n", "        y_train = train_df['zigzag_label'].values\n", "        X_val = val_df[selected_features].values.astype(np.float32)\n", "        y_val = val_df['zigzag_label'].values\n", "        X_test = test_df[selected_features].values.astype(np.float32)\n", "        y_test = test_df['zigzag_label'].values\n", "        \n", "        # Normalize\n", "        mean = X_train.mean(axis=0)\n", "        std = X_train.std(axis=0) + 1e-8\n", "        X_train = (X_train - mean) / std\n", "        X_val = (X_val - mean) / std\n", "        X_test = (X_test - mean) / std\n", "        \n", "        # Sequences\n", "        def create_seq(X, y):\n", "            Xs, ys = [], []\n", "            for i in range(len(X) - 20):\n", "                Xs.append(X[i:i+20])\n", "                ys.append(y[i+20])\n", "            return np.array(Xs, dtype=np.float32), np.array(ys)\n", "        \n", "        X_train_seq, y_train_seq = create_seq(X_train, y_train)\n", "        X_val_seq, y_val_seq = create_seq(X_val, y_val)\n", "        X_test_seq, y_test_seq = create_seq(X_test, y_test)\n", "        \n", "        print(f'  âœ“ Sequences: {X_train_seq.shape}')\n", "        \n", "        # Class weights\n", "        unique, counts = np.unique(y_train_seq, return_counts=True)\n", "        total = len(y_train_seq)\n", "        class_weights = {}\n", "        for u, c in zip(unique, counts):\n", "            if u == 0:\n", "                class_weights[u] = 1.0\n", "            else:\n", "                class_weights[u] = total / (5 * c) * 3\n", "        \n", "        # Binary labels\n", "        y_train_binary = (y_train_seq != 0).astype(np.float32)\n", "        y_val_binary = (y_val_seq != 0).astype(np.float32)\n", "        y_test_binary = (y_test_seq != 0).astype(np.float32)\n", "        \n", "        # Train Classification Model\n", "        print(f'Training classification model...')\n", "        clf_model = build_classification_model((X_train_seq.shape[1], X_train_seq.shape[2]))\n", "        \n", "        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)\n", "        \n", "        clf_model.fit(\n", "            X_train_seq, y_train_seq,\n", "            validation_data=(X_val_seq, y_val_seq),\n", "            epochs=200,\n", "            batch_size=32,\n", "            class_weight=class_weights,\n", "            callbacks=[early_stop],\n", "            verbose=0\n", "        )\n", "        \n", "        clf_loss, clf_acc = clf_model.evaluate(X_test_seq, y_test_seq, verbose=0)\n", "        print(f'  âœ“ Classification Test Acc: {clf_acc:.4f}')\n", "        \n", "        # Train Detection Model\n", "        print(f'Training detection model...')\n", "        det_model = build_detection_model((X_train_seq.shape[1], X_train_seq.shape[2]))\n", "        \n", "        det_model.fit(\n", "            X_train_seq, y_train_binary,\n", "            validation_data=(X_val_seq, y_val_binary),\n", "            epochs=100,\n", "            batch_size=32,\n", "            callbacks=[early_stop],\n", "            verbose=0\n", "        )\n", "        \n", "        det_loss, det_acc = det_model.evaluate(X_test_seq, y_test_binary, verbose=0)\n", "        print(f'  âœ“ Detection Test Acc: {det_acc:.4f}')\n", "        \n", "        # Save models\n", "        model_dir = f'{project_root}/models/{symbol.lower()}_{timeframe}'\n", "        os.makedirs(model_dir, exist_ok=True)\n", "        \n", "        clf_model.save(f'{model_dir}/classification.h5')\n", "        det_model.save(f'{model_dir}/detection.h5')\n", "        \n", "        # Save parameters\n", "        params = {\n", "            'mean': mean.tolist(),\n", "            'std': std.tolist(),\n", "            'class_weights': {int(k): v for k, v in class_weights.items()},\n", "            'clf_acc': float(clf_acc),\n", "            'det_acc': float(det_acc)\n", "        }\n", "        with open(f'{model_dir}/params.json', 'w') as f:\n", "            json.dump(params, f, indent=2)\n", "        \n", "        print(f'  âœ“ Models saved to {model_dir}')\n", "        \n", "        return {\n", "            'symbol': symbol,\n", "            'timeframe': timeframe,\n", "            'clf_acc': clf_acc,\n", "            'det_acc': det_acc,\n", "            'status': 'success'\n", "        }\n", "    \n", "    except Exception as e:\n", "        print(f'  âœ— Error: {str(e)}')\n", "        return {\n", "            'symbol': symbol,\n", "            'timeframe': timeframe,\n", "            'status': 'failed',\n", "            'error': str(e)\n", "        }\n", "\n", "print('Training function ready')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5: Batch Training\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fetcher = CryptoDataFetcher()\n", "zigzag = ZigZagIndicator(depth=12, deviation=5, backstep=2)\n", "fe = FeatureEngineer(lookback_periods=[5, 10, 20, 50, 200])\n", "\n", "results = []\n", "\n", "for symbol in SYMBOLS:\n", "    for timeframe in TIMEFRAMES:\n", "        result = train_symbol_timeframe(symbol, timeframe, fetcher, zigzag, fe)\n", "        if result:\n", "            results.append(result)\n", "\n", "print(f'\\n{'='*60}')\n", "print('BATCH TRAINING COMPLETE')\n", "print(f'{'='*60}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 6: Summary Report\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Save results\n", "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n", "results_file = f'{project_root}/training_logs/batch_results_{timestamp}.json'\n", "\n", "with open(results_file, 'w') as f:\n", "    json.dump(results, f, indent=2)\n", "\n", "print(f'Results saved to: {results_file}')\n", "\n", "# Summary\n", "successful = [r for r in results if r['status'] == 'success']\n", "failed = [r for r in results if r['status'] == 'failed']\n", "\n", "print(f'\\nSuccessful: {len(successful)}/{len(results)}')\n", "print(f'Failed: {len(failed)}/{len(results)}')\n", "\n", "if successful:\n", "    avg_clf = np.mean([r['clf_acc'] for r in successful])\n", "    avg_det = np.mean([r['det_acc'] for r in successful])\n", "    print(f'\\nAverage Classification Accuracy: {avg_clf:.4f}')\n", "    print(f'Average Detection Accuracy: {avg_det:.4f}')\n", "    \n", "    print(f'\\nTop 5 Classification Models:')\n", "    sorted_clf = sorted(successful, key=lambda x: x['clf_acc'], reverse=True)[:5]\n", "    for r in sorted_clf:\n", "        print(f'  {r[\"symbol\"]} {r[\"timeframe\"]}: {r[\"clf_acc\"]:.4f}')\n", "\n", "if failed:\n", "    print(f'\\nFailed Models:')\n", "    for r in failed:\n", "        print(f'  {r[\"symbol\"]} {r[\"timeframe\"]}: {r.get(\"error\", \"Unknown error\")}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 7: Verify Models\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "model_base = f'{project_root}/models'\n", "model_dirs = [d for d in os.listdir(model_base) if os.path.isdir(os.path.join(model_base, d))]\n", "\n", "print(f'Total model directories: {len(model_dirs)}')\n", "print(f'Expected: {len(SYMBOLS) * len(TIMEFRAMES)} (22 Ã— 2)')\n", "\n", "# Check model files\n", "complete = 0\n", "for model_dir in model_dirs:\n", "    path = os.path.join(model_base, model_dir)\n", "    has_clf = os.path.exists(os.path.join(path, 'classification.h5'))\n", "    has_det = os.path.exists(os.path.join(path, 'detection.h5'))\n", "    has_params = os.path.exists(os.path.join(path, 'params.json'))\n", "    \n", "    if has_clf and has_det and has_params:\n", "        complete += 1\n", "\n", "print(f'\\nComplete model sets: {complete}/{len(model_dirs)}')\n", "print(f'Total model files: {complete * 3} (classification + detection + params)')\n", "\n", "print(f'\\nModel structure:')\n", "print(f'ðŸ“ models/')\n", "print(f'  â”œâ”€â”€ btcusdt_15m/')\n", "print(f'  â”‚   â”œâ”€â”€ classification.h5 (5-class signal classifier)')\n", "print(f'  â”‚   â”œâ”€â”€ detection.h5 (binary signal detector)')\n", "print(f'  â”‚   â””â”€â”€ params.json (normalization params)')\n", "print(f'  â”œâ”€â”€ btcusdt_1h/')\n", "print(f'  â”‚   â””â”€â”€ ...')\n", "print(f'  â””â”€â”€ ... (22 symbols Ã— 2 timeframes = 44 model sets)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Done!\n", "\n", "All models are trained and saved to Google Drive.\n", "You can now download them and deploy to production!\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [""]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11.0"}}, "nbformat": 4, "nbformat_minor": 5}